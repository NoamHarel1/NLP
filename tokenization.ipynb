{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:43.966643Z",
     "start_time": "2024-06-01T11:14:43.962310Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *Statistics:*",
   "id": "fe949615f481505d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:43.975339Z",
     "start_time": "2024-06-01T11:14:43.973136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path, encoding='latin-1')\n",
    "    df.columns = ['v1', 'v2', 'v3', 'v4', 'v5']\n",
    "    return df"
   ],
   "id": "f2bab60cc46b73c1",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:43.999158Z",
     "start_time": "2024-06-01T11:14:43.994707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_basic_statistics(df: pd.DataFrame):\n",
    "\n",
    "    total_messages = df.shape[0]\n",
    "\n",
    "    spam_count = df[df['v1'] == 'spam'].shape[0]\n",
    "    ham_count = df[df['v1'] == 'ham'].shape[0]\n",
    "\n",
    "    df['word_count'] = df['v2'].apply(lambda x: len(x.split()))\n",
    "    avg_words_per_message = df['word_count'].mean()\n",
    "\n",
    "    all_words = ' '.join(df['v2']).split()\n",
    "    most_common_words = Counter(all_words).most_common(5)\n",
    "\n",
    "    word_counts = Counter(all_words)\n",
    "    words_once = sum(1 for count in word_counts.values() if count == 1)\n",
    "\n",
    "    print(f'Total number of SMS messages: {total_messages}')\n",
    "    print(f'Number of spam messages: {spam_count}')\n",
    "    print(f'Number of ham messages: {ham_count}')\n",
    "    print(f'Average number of words per message: {avg_words_per_message:.2f}')\n",
    "    print('5 most frequent words:', most_common_words)\n",
    "    print(f'Number of words that only appear once: {words_once}')\n"
   ],
   "id": "262154602765fb2f",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *NLP Tokenization and Lemmatization:*\n",
   "id": "80002c0941fb8975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.729164Z",
     "start_time": "2024-06-01T11:14:44.000595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ],
   "id": "68202143644bbfe2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *NLTK:*",
   "id": "907fa41d7dc7de9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.731851Z",
     "start_time": "2024-06-01T11:14:44.729850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_nltk(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def lemmatize_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n"
   ],
   "id": "fd0cdbe042093853",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *spaCy:*",
   "id": "97103cb2271be2d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.735189Z",
     "start_time": "2024-06-01T11:14:44.733240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def lemmatize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n"
   ],
   "id": "78f05f00ae660424",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *Time Analysis:*",
   "id": "4fa6b918536d971c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.738056Z",
     "start_time": "2024-06-01T11:14:44.735995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_tokenization_time(df, tokenizer, tokenizer_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{tokenizer_name}_tokens'\n",
    "    df[col_name] = df['v2'].apply(tokenizer)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Tokenization with {tokenizer_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "d210a6dccd098873",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.741026Z",
     "start_time": "2024-06-01T11:14:44.738910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_lemmatization_time(df, lemmatization, lemmatization_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{lemmatization_name}_lemmatization'\n",
    "    df[col_name] = df['v2'].apply(lemmatization)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Lemmatization_name with {lemmatization_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "4306db45581eaf76",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.743777Z",
     "start_time": "2024-06-01T11:14:44.741716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_tokenize(df: pd.DataFrame):\n",
    "    analyze_tokenization_time(df, tokenize_nltk, 'nltk')\n",
    "    analyze_tokenization_time(df, tokenize_spacy, 'spaCy')\n",
    "    df.to_csv('spam_tokenized.csv')"
   ],
   "id": "81a9eaff3359b67b",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.746535Z",
     "start_time": "2024-06-01T11:14:44.744533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_lemmatization(df: pd.DataFrame):\n",
    "    analyze_lemmatization_time(df, lemmatize_nltk, 'nltk')\n",
    "    analyze_lemmatization_time(df, lemmatize_spacy, 'spaCy')\n",
    "    df.to_csv('spam_lemmatize.csv')"
   ],
   "id": "85b30e5707d4165f",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *NLTK Analyze:*\n",
    "\n",
    "    The output of the NLTK tokenization is a list of tokens for each sentence, it is a simple tokenization process that splits the text by space and punctuation, for example it will split \"I'm\" into \"I\" and \"'m\".\n",
    "    The output of the NLTK lemmatization is a list of lemmatized tokens for each sentence, it is a simple lemmatization process that reduces the words to their base form, In nltk it didnt success to reduce the word to its form, for example \"searching\" stay \"searching\", It didnt preform well.\n",
    "    The proccessing speed is very fast, in this case it took 0.3 seconds. \n",
    "    It is primarily designed for english.\n",
    "    The complexity for tokenizing each row is ùëÇ(ùëõ) therefore tokenizing the entire file would be ùëÇ(ùëö‚ãÖùëõ), where ùëö is the average length of the text and ùëõ is the number of rows.\n",
    "    \n",
    "### *spaCy Analyze:*\n",
    "\n",
    "    The output of the spaCy tokenization is a list of tokens for each sentence, it is a more complex tokenization process that takes into account the context of the words, for example it will not split \"I'm\" into \"I\" and \"'m\" and will tokenize it into \"I'm\".\n",
    "    The output of the spaCy lemmatization is a list of lemmatized tokens for each sentence, it is a simple lemmatization process that reduces the words to their base form, for example it will reduce \"running\" to \"run\".\n",
    "    The proccessing speed is slower than NLTK, in this case it took 313 seconds.\n",
    "    it suppport various languages.\n",
    "    The complexity for tokenizing each row is ùëÇ(ùëõ) therefore tokenizing the entire file would be ùëÇ(ùëö‚ãÖùëõ), where ùëö is the average length of the text and ùëõ is the number of rows."
   ],
   "id": "9769669353e9123d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.766255Z",
     "start_time": "2024-06-01T11:14:44.747410Z"
    }
   },
   "cell_type": "code",
   "source": "spam_df = load_data('spam.csv')",
   "id": "cbcb00659cddede2",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:14:44.793648Z",
     "start_time": "2024-06-01T11:14:44.768943Z"
    }
   },
   "cell_type": "code",
   "source": "print_basic_statistics(spam_df)",
   "id": "6ba727e6f46d0f49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of SMS messages: 5572\n",
      "Number of spam messages: 747\n",
      "Number of ham messages: 4825\n",
      "Average number of words per message: 15.49\n",
      "5 most frequent words: [('to', 2134), ('you', 1622), ('I', 1466), ('a', 1327), ('the', 1197)]\n",
      "Number of words that only appear once: 9268\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:15:14.462406Z",
     "start_time": "2024-06-01T11:14:44.794622Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_tokenize(spam_df)",
   "id": "ad9aa95cc085002a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization with nltk took 0.3706 seconds\n",
      "Tokenization with spaCy took 29.2516 seconds\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T11:15:43.326018Z",
     "start_time": "2024-06-01T11:15:14.463090Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_lemmatization(spam_df)",
   "id": "6a6022ccd225449d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization_name with nltk took 0.5819 seconds\n",
      "Lemmatization_name with spaCy took 28.2079 seconds\n"
     ]
    }
   ],
   "execution_count": 71
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
