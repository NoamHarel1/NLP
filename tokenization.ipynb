{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T08:45:12.670032Z",
     "start_time": "2024-06-01T08:45:11.857919Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:45:13.128508Z",
     "start_time": "2024-06-01T08:45:12.671259Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('punkt')",
   "id": "68202143644bbfe2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:45:13.131429Z",
     "start_time": "2024-06-01T08:45:13.129376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_nltk(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "def tokenize_spacy(text):\n",
    "    nlp = English()\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "    tokens = tokenizer(text)\n",
    "\n",
    "    return list(tokens)\n"
   ],
   "id": "fd0cdbe042093853",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:45:13.134307Z",
     "start_time": "2024-06-01T08:45:13.132445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_tokenization_time(df, tokenizer, tokenizer_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{tokenizer_name}_tokens'\n",
    "    df[col_name] = df['v2'].apply(tokenizer)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Tokenization with {tokenizer_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "d210a6dccd098873",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:45:13.136714Z",
     "start_time": "2024-06-01T08:45:13.134903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_tokenize(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path, encoding='latin-1')\n",
    "    df.columns = ['v1', 'v2', 'v3', 'v4', 'v5']\n",
    "\n",
    "    analyze_tokenization_time(df, tokenize_nltk, 'nltk')\n",
    "\n",
    "    analyze_tokenization_time(df, tokenize_spacy, 'spaCy')\n",
    "    df.to_csv('spam_tokenized.csv')"
   ],
   "id": "81a9eaff3359b67b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "NLTK Tokenization:\n",
    "\n",
    "    The output of the NLTK tokenization is a list of words for each sentence, it is a simple tokenization process that splits the text by space and punctuation, for example it will split \"I'm\" into \"I\" and \"'m\".\n",
    "    The proccessing speed is very fast, in this case it took 0.3 seconds. \n",
    "    it suppport various languages.\n",
    "    The complexity for tokenizing each row is ùëÇ(ùëõ) therefore tokenizing the entire file would be ùëÇ(ùëö‚ãÖùëõ), where ùëõ is the average length of the text and ùëö is the number of rows.\n",
    "    \n",
    "spaCy Tokenization:\n",
    "\n",
    "    The output of the spaCy tokenization is a list of tokens for each sentence, it is a more complex tokenization process that takes into account the context of the words, for example it will not split \"I'm\" into \"I\" and \"'m\" and will tokenize it into \"I'm\"."
   ],
   "id": "9769669353e9123d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T08:50:21.661364Z",
     "start_time": "2024-06-01T08:45:13.137610Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_tokenize('spam.csv')",
   "id": "ad9aa95cc085002a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization with nltk took 0.3683 seconds\n",
      "Tokenization with spaCy took 306.5064 seconds\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
