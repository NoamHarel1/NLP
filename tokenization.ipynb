{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.637676Z",
     "start_time": "2024-06-01T12:12:45.586347Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *Statistics:*",
   "id": "fe949615f481505d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.641089Z",
     "start_time": "2024-06-01T12:12:45.639103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path, encoding='latin-1')\n",
    "    df.columns = ['v1', 'v2', 'v3', 'v4', 'v5']\n",
    "    return df"
   ],
   "id": "f2bab60cc46b73c1",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.653078Z",
     "start_time": "2024-06-01T12:12:45.648528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_total_sms(df: pd.DataFrame):\n",
    "    total_messages = df.shape[0]\n",
    "    print(f'Total number of SMS messages: {total_messages}')\n",
    "    \n",
    "def print_spam_ham_ratio(df: pd.DataFrame):\n",
    "    spam_count = df[df['v1'] == 'spam'].shape[0]\n",
    "    ham_count = df[df['v1'] == 'ham'].shape[0]\n",
    "    print(f'Spam to ham ratio: {spam_count / ham_count:.2f}')\n",
    "\n",
    "def average_word_length(df: pd.DataFrame, column_name):\n",
    "    df['word_length'] = df[column_name].apply(lambda x: len(x))\n",
    "    avg_word_length = df['word_length'].mean()\n",
    "    print(f'Average word length: {avg_word_length:.2f}')\n",
    "    \n",
    "def most_common_words(df: pd.DataFrame, column_name):\n",
    "    all_words = ' '.join(df[column_name]).split()\n",
    "    most_common = Counter(all_words).most_common(5)\n",
    "    print('5 most frequent words:', most_common)\n",
    "\n",
    "def number_of_words_once(df: pd.DataFrame, column_name):\n",
    "    all_words = ' '.join(df[column_name]).split()\n",
    "    word_counts = Counter(all_words)\n",
    "    words_once = sum(1 for count in word_counts.values() if count == 1)\n",
    "    print(f'Number of words that only appear once: {words_once}')\n",
    "\n",
    "def print_basic_statistics(df: pd.DataFrame):\n",
    "    print_total_sms(df)\n",
    "    print_spam_ham_ratio(df)\n",
    "    average_word_length(df, 'v2')\n",
    "    most_common_words(df, 'v2')\n",
    "    number_of_words_once(df, 'v2')\n",
    "\n",
    "def print_statistics_after_applying_technique(df: pd.DataFrame, column_name):\n",
    "    average_word_length(df, column_name)\n",
    "    most_common_words(df, column_name)\n"
   ],
   "id": "262154602765fb2f",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *NLP Tokenization and Lemmatization:*\n",
   "id": "80002c0941fb8975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.906344Z",
     "start_time": "2024-06-01T12:12:45.654219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ],
   "id": "68202143644bbfe2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *NLTK:*",
   "id": "907fa41d7dc7de9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.909856Z",
     "start_time": "2024-06-01T12:12:45.907677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_nltk(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def lemmatize_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def stem_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens]\n"
   ],
   "id": "fd0cdbe042093853",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *spaCy:*",
   "id": "97103cb2271be2d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.912571Z",
     "start_time": "2024-06-01T12:12:45.910722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def lemmatize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n"
   ],
   "id": "78f05f00ae660424",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *Time Analysis:*",
   "id": "4fa6b918536d971c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.916063Z",
     "start_time": "2024-06-01T12:12:45.913943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_tokenization_time(df, tokenizer, tokenizer_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{tokenizer_name}_tokens'\n",
    "    df[col_name] = df['v2'].apply(tokenizer)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Tokenization with {tokenizer_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "d210a6dccd098873",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.919018Z",
     "start_time": "2024-06-01T12:12:45.916870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_lemmatization_time(df, lemmatization, lemmatization_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{lemmatization_name}_lemmatization'\n",
    "    df[col_name] = df['v2'].apply(lemmatization)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Lemmatization with {lemmatization_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "4306db45581eaf76",
   "outputs": [],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.921853Z",
     "start_time": "2024-06-01T12:12:45.919682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_stemming_time(df, stemming, stemming_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{stemming_name}_stemming'\n",
    "    df[col_name] = df['v2'].apply(stemming)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Stemming with {stemming_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "7c7165996b6fee3d",
   "outputs": [],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.924298Z",
     "start_time": "2024-06-01T12:12:45.922398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_tokenize(df: pd.DataFrame):\n",
    "    analyze_tokenization_time(df, tokenize_nltk, 'nltk')\n",
    "    analyze_tokenization_time(df, tokenize_spacy, 'spaCy')\n",
    "    print_statistics_after_applying_technique(df, 'nltk_tokens')\n",
    "    print_statistics_after_applying_technique(df, 'spaCy_tokens')\n",
    "    df.to_csv('spam_tokenized.csv')"
   ],
   "id": "81a9eaff3359b67b",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.927008Z",
     "start_time": "2024-06-01T12:12:45.924848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_lemmatization(df: pd.DataFrame):\n",
    "    analyze_lemmatization_time(df, lemmatize_nltk, 'nltk')\n",
    "    analyze_lemmatization_time(df, lemmatize_spacy, 'spaCy')\n",
    "    print_statistics_after_applying_technique(df, 'nltk_lemmatization')\n",
    "    print_statistics_after_applying_technique(df, 'spaCy_lemmatization')\n",
    "\n",
    "    df.to_csv('spam_lemmatize.csv')"
   ],
   "id": "85b30e5707d4165f",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.929731Z",
     "start_time": "2024-06-01T12:12:45.927848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_stemming(df: pd.DataFrame):\n",
    "    analyze_stemming_time(df, stem_nltk, 'nltk')\n",
    "    analyze_stemming_time(df, lemmatize_spacy, 'spaCy')\n",
    "    print_statistics_after_applying_technique(df, 'nltk_stemming')\n",
    "    print_statistics_after_applying_technique(df, 'spaCy_stemming')\n",
    "\n",
    "    df.to_csv('spam_stemming.csv')"
   ],
   "id": "63fb8486dbc5dec4",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *NLTK Analyze:*\n",
    "\n",
    "    The output of the NLTK tokenization is a list of tokens for each sentence, it is a simple tokenization process that splits the text by space and punctuation, for example it will split \"I'm\" into \"I\" and \"'m\".\n",
    "    The output of the NLTK lemmatization is a list of lemmatized tokens for each sentence, it is a simple lemmatization process that reduces the words to their base form, In nltk it didnt success to reduce the word to its form, for example \"searching\" stay \"searching\", It didnt preform well.\n",
    "    we used the porter stemmer for stemming the text.  stemming algorithms are known for their simplicity and effectiveness. It applies a series of rules to iteratively strip suffixes from words.\n",
    "    The proccessing speed is very fast, in this case it took 0.3 seconds but the result is not accurate. \n",
    "    It is primarily designed for english.\n",
    "    The complexity for tokenizing/lemmatization/stemming each row is ð‘‚(ð‘›) therefore tokenizing the entire file would be ð‘‚(ð‘šâ‹…ð‘›), where ð‘š is the average length of the text and ð‘› is the number of rows.\n",
    "    \n",
    "### *spaCy Analyze:*\n",
    "\n",
    "    The output of the spaCy tokenization is a list of tokens for each sentence, it is a more complex tokenization process that takes into account the context of the words, for example it will not split \"I'm\" into \"I\" and \"'m\" and will tokenize it into \"I'm\".\n",
    "    The output of the spaCy lemmatization is a list of lemmatized tokens for each sentence, it is a simple lemmatization process that reduces the words to their base form, for example it will reduce \"running\" to \"run\".\n",
    "    SpaCy doesn't contain any function for stemming as it relies on lemmatization only\n",
    "    The proccessing speed is slower than NLTK, in this case it took 313 seconds and the result is more accurate than NLTK.\n",
    "    it suppport various languages.\n",
    "    The complexity for tokenizing/lemmatization/stemming each row is ð‘‚(ð‘›) therefore tokenizing the entire file would be ð‘‚(ð‘šâ‹…ð‘›), where ð‘š is the average length of the text and ð‘› is the number of rows."
   ],
   "id": "9769669353e9123d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:45.973883Z",
     "start_time": "2024-06-01T12:12:45.931731Z"
    }
   },
   "cell_type": "code",
   "source": "spam_df = load_data('spam.csv')",
   "id": "cbcb00659cddede2",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:12:46.002322Z",
     "start_time": "2024-06-01T12:12:45.974873Z"
    }
   },
   "cell_type": "code",
   "source": "print_basic_statistics(spam_df)",
   "id": "6ba727e6f46d0f49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of SMS messages: 5572\n",
      "Spam to ham ratio: 0.15\n",
      "Average word length: 80.12\n",
      "5 most frequent words: [('to', 2134), ('you', 1622), ('I', 1466), ('a', 1327), ('the', 1197)]\n",
      "Number of words that only appear once: 9268\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:13:15.573741Z",
     "start_time": "2024-06-01T12:12:46.002973Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_tokenize(spam_df)",
   "id": "ad9aa95cc085002a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization with nltk took 0.3815 seconds\n",
      "Tokenization with spaCy took 29.1611 seconds\n",
      "Average word length: 18.70\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[120], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mload_data_and_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspam_df\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[115], line 4\u001B[0m, in \u001B[0;36mload_data_and_tokenize\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m      2\u001B[0m analyze_tokenization_time(df, tokenize_nltk, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnltk\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m analyze_tokenization_time(df, tokenize_spacy, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspaCy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mprint_statistics_after_applying_technique\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnltk_tokens\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m print_statistics_after_applying_technique(df, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspaCy_tokens\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m df\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspam_tokenized.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[108], line 35\u001B[0m, in \u001B[0;36mprint_statistics_after_applying_technique\u001B[0;34m(df, column_name)\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprint_statistics_after_applying_technique\u001B[39m(df: pd\u001B[38;5;241m.\u001B[39mDataFrame, column_name):\n\u001B[1;32m     34\u001B[0m     average_word_length(df, column_name)\n\u001B[0;32m---> 35\u001B[0m     \u001B[43mmost_common_words\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[108], line 16\u001B[0m, in \u001B[0;36mmost_common_words\u001B[0;34m(df, column_name)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmost_common_words\u001B[39m(df: pd\u001B[38;5;241m.\u001B[39mDataFrame, column_name):\n\u001B[0;32m---> 16\u001B[0m     all_words \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcolumn_name\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msplit()\n\u001B[1;32m     17\u001B[0m     most_common \u001B[38;5;241m=\u001B[39m Counter(all_words)\u001B[38;5;241m.\u001B[39mmost_common(\u001B[38;5;241m5\u001B[39m)\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m5 most frequent words:\u001B[39m\u001B[38;5;124m'\u001B[39m, most_common)\n",
      "\u001B[0;31mTypeError\u001B[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_data_and_lemmatization(spam_df)",
   "id": "6a6022ccd225449d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_data_and_stemming(spam_df)",
   "id": "cef6d7d4d6bbac5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ",
   "id": "32410af67dfffa8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Replace this with the URL of the public page you want to scrape\n",
    "url = 'https://example.com/public-profile'\n",
    "\n",
    "def scrape_text_from_profile(url):\n",
    "    # Fetch the HTML content of the page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract text data\n",
    "    # Adjust the tag and class names based on the structure of the page you are scraping\n",
    "    # For example, to extract all paragraphs:\n",
    "    text_data = []\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        text_data.append(paragraph.get_text())\n",
    "\n",
    "    # Print the extracted text data\n",
    "    for text in text_data:\n",
    "        print(text)\n",
    "\n",
    "# Example usage\n",
    "scrape_text_from_profile(url)\n"
   ],
   "id": "2dc7cb148a7cd0be",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
