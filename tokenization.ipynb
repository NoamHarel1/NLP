{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:53.939259Z",
     "start_time": "2024-06-01T12:28:53.937091Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ],
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *Statistics:*",
   "id": "fe949615f481505d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.008080Z",
     "start_time": "2024-06-01T12:28:54.005976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path, encoding='latin-1')\n",
    "    df.columns = ['v1', 'v2', 'v3', 'v4', 'v5']\n",
    "    return df"
   ],
   "id": "f2bab60cc46b73c1",
   "outputs": [],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.013524Z",
     "start_time": "2024-06-01T12:28:54.009393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_total_sms(df: pd.DataFrame):\n",
    "    total_messages = df.shape[0]\n",
    "    print(f'Total number of SMS messages: {total_messages}')\n",
    "    \n",
    "def print_spam_ham_ratio(df: pd.DataFrame):\n",
    "    spam_count = df[df['v1'] == 'spam'].shape[0]\n",
    "    ham_count = df[df['v1'] == 'ham'].shape[0]\n",
    "    print(f'Spam to ham ratio: {spam_count / ham_count:.2f}')\n",
    "\n",
    "def average_word_length(df: pd.DataFrame, column_name):\n",
    "    df['word_length'] = df[column_name].apply(lambda x: len(x))\n",
    "    avg_word_length = df['word_length'].mean()\n",
    "    print(f'Average word length: {avg_word_length:.2f}')\n",
    "    \n",
    "def most_common_words(df: pd.DataFrame, column_name):\n",
    "    all_words = ' '.join(df[column_name]).split()\n",
    "    most_common = Counter(all_words).most_common(5)\n",
    "    print('5 most frequent words:', most_common)\n",
    "\n",
    "def number_of_words_once(df: pd.DataFrame, column_name):\n",
    "    all_words = ' '.join(df[column_name]).split()\n",
    "    word_counts = Counter(all_words)\n",
    "    words_once = sum(1 for count in word_counts.values() if count == 1)\n",
    "    print(f'Number of words that only appear once: {words_once}')\n",
    "\n",
    "def print_basic_statistics(df: pd.DataFrame):\n",
    "    print_total_sms(df)\n",
    "    print_spam_ham_ratio(df)\n",
    "    average_word_length(df, 'v2')\n",
    "    most_common_words(df, 'v2')\n",
    "    number_of_words_once(df, 'v2')\n",
    "\n",
    "def print_statistics_after_applying_technique(df: pd.DataFrame, column_name):\n",
    "    average_word_length(df, column_name)\n",
    "    most_common_words(df, column_name)\n"
   ],
   "id": "262154602765fb2f",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *NLP Tokenization and Lemmatization:*\n",
   "id": "80002c0941fb8975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.255118Z",
     "start_time": "2024-06-01T12:28:54.014542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ],
   "id": "68202143644bbfe2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/noam/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *NLTK:*",
   "id": "907fa41d7dc7de9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.258527Z",
     "start_time": "2024-06-01T12:28:54.256145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_nltk(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def lemmatize_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def stem_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens]\n"
   ],
   "id": "fd0cdbe042093853",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *spaCy:*",
   "id": "97103cb2271be2d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.262199Z",
     "start_time": "2024-06-01T12:28:54.259984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def lemmatize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n"
   ],
   "id": "78f05f00ae660424",
   "outputs": [],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *Time Analysis:*",
   "id": "4fa6b918536d971c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.265018Z",
     "start_time": "2024-06-01T12:28:54.263083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_tokenization_time(df, tokenizer, tokenizer_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{tokenizer_name}_tokens'\n",
    "    df[col_name] = df['v2'].apply(tokenizer)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Tokenization with {tokenizer_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "d210a6dccd098873",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.268128Z",
     "start_time": "2024-06-01T12:28:54.265979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_lemmatization_time(df, lemmatization, lemmatization_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{lemmatization_name}_lemmatization'\n",
    "    df[col_name] = df['v2'].apply(lemmatization)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Lemmatization with {lemmatization_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "4306db45581eaf76",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.271087Z",
     "start_time": "2024-06-01T12:28:54.269082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_stemming_time(df, stemming, stemming_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{stemming_name}_stemming'\n",
    "    df[col_name] = df['v2'].apply(stemming)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Stemming with {stemming_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "7c7165996b6fee3d",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.274058Z",
     "start_time": "2024-06-01T12:28:54.271677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_tokenize(df: pd.DataFrame):\n",
    "    analyze_tokenization_time(df, tokenize_nltk, 'nltk')\n",
    "    analyze_tokenization_time(df, tokenize_spacy, 'spaCy')\n",
    "    printed_df = df.copy()\n",
    "    printed_df['nltk_tokens'] = printed_df['nltk_tokens'].apply(lambda x: ' '.join(x))\n",
    "    printed_df['spaCy_tokens'] = printed_df['spaCy_tokens'].apply(lambda x: ' '.join(x))\n",
    "    print_statistics_after_applying_technique(printed_df, 'nltk_tokens')\n",
    "    print_statistics_after_applying_technique(printed_df, 'spaCy_tokens')\n",
    "    df.to_csv('spam_tokenized.csv')"
   ],
   "id": "81a9eaff3359b67b",
   "outputs": [],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.278366Z",
     "start_time": "2024-06-01T12:28:54.276238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_lemmatization(df: pd.DataFrame):\n",
    "    analyze_lemmatization_time(df, lemmatize_nltk, 'nltk')\n",
    "    analyze_lemmatization_time(df, lemmatize_spacy, 'spaCy')\n",
    "    printed_df = df.copy()\n",
    "    printed_df['nltk_lemmatization'] = printed_df['nltk_lemmatization'].apply(lambda x: ' '.join(x))\n",
    "    printed_df['spaCy_lemmatization'] = printed_df['spaCy_lemmatization'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    print_statistics_after_applying_technique(printed_df, 'nltk_lemmatization')\n",
    "    print_statistics_after_applying_technique(printed_df, 'spaCy_lemmatization')\n",
    "\n",
    "    df.to_csv('spam_lemmatize.csv')"
   ],
   "id": "85b30e5707d4165f",
   "outputs": [],
   "execution_count": 192
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.281076Z",
     "start_time": "2024-06-01T12:28:54.279019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_stemming(df: pd.DataFrame):\n",
    "    analyze_stemming_time(df, stem_nltk, 'nltk')\n",
    "    analyze_stemming_time(df, lemmatize_spacy, 'spaCy')\n",
    "    printed_df = df.copy()\n",
    "    printed_df['nltk_stemming'] = printed_df['nltk_stemming'].apply(lambda x: ' '.join(x))\n",
    "    printed_df['spaCy_stemming'] = printed_df['spaCy_stemming'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    print_statistics_after_applying_technique(printed_df, 'nltk_stemming')\n",
    "    print_statistics_after_applying_technique(printed_df, 'spaCy_stemming')\n",
    "\n",
    "    df.to_csv('spam_stemming.csv')"
   ],
   "id": "63fb8486dbc5dec4",
   "outputs": [],
   "execution_count": 193
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *NLTK Analyze:*\n",
    "\n",
    "    The output of the NLTK tokenization is a list of tokens for each sentence, it is a simple tokenization process that splits the text by space and punctuation, for example it will split \"I'm\" into \"I\" and \"'m\".\n",
    "    The output of the NLTK lemmatization is a list of lemmatized tokens for each sentence, it is a simple lemmatization process that reduces the words to their base form, In nltk it didnt success to reduce the word to its form, for example \"searching\" stay \"searching\", It didnt preform well.\n",
    "    we used the porter stemmer for stemming the text.  stemming algorithms are known for their simplicity and effectiveness. It applies a series of rules to iteratively strip suffixes from words.\n",
    "    The proccessing speed is very fast, in this case it took 0.3 seconds but the result is not accurate. \n",
    "    It is primarily designed for english.\n",
    "    The complexity for tokenizing/lemmatization/stemming each row is 𝑂(𝑛) therefore tokenizing the entire file would be 𝑂(𝑚⋅𝑛), where 𝑚 is the average length of the text and 𝑛 is the number of rows.\n",
    "    \n",
    "### *spaCy Analyze:*\n",
    "\n",
    "    The output of the spaCy tokenization is a list of tokens for each sentence, it is a more complex tokenization process that takes into account the context of the words, for example it will not split \"I'm\" into \"I\" and \"'m\" and will tokenize it into \"I'm\".\n",
    "    The output of the spaCy lemmatization is a list of lemmatized tokens for each sentence, it is a simple lemmatization process that reduces the words to their base form, for example it will reduce \"running\" to \"run\".\n",
    "    SpaCy doesn't contain any function for stemming as it relies on lemmatization only\n",
    "    The proccessing speed is slower than NLTK, in this case it took 313 seconds and the result is more accurate than NLTK.\n",
    "    it suppport various languages.\n",
    "    The complexity for tokenizing/lemmatization/stemming each row is 𝑂(𝑛) therefore tokenizing the entire file would be 𝑂(𝑚⋅𝑛), where 𝑚 is the average length of the text and 𝑛 is the number of rows."
   ],
   "id": "9769669353e9123d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.290634Z",
     "start_time": "2024-06-01T12:28:54.281865Z"
    }
   },
   "cell_type": "code",
   "source": "spam_df = load_data('spam.csv')",
   "id": "cbcb00659cddede2",
   "outputs": [],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:28:54.315292Z",
     "start_time": "2024-06-01T12:28:54.291376Z"
    }
   },
   "cell_type": "code",
   "source": "print_basic_statistics(spam_df)",
   "id": "6ba727e6f46d0f49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of SMS messages: 5572\n",
      "Spam to ham ratio: 0.15\n",
      "Average word length: 80.12\n",
      "5 most frequent words: [('to', 2134), ('you', 1622), ('I', 1466), ('a', 1327), ('the', 1197)]\n",
      "Number of words that only appear once: 9268\n"
     ]
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:29:23.744497Z",
     "start_time": "2024-06-01T12:28:54.316204Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_tokenize(spam_df)",
   "id": "ad9aa95cc085002a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization with nltk took 0.3830 seconds\n",
      "Tokenization with spaCy took 28.9720 seconds\n",
      "Average word length: 83.26\n",
      "5 most frequent words: [('.', 4886), ('to', 2148), ('I', 1956), ('you', 1888), (',', 1871)]\n",
      "Average word length: 83.17\n",
      "5 most frequent words: [('.', 4945), ('to', 2148), ('I', 1988), ('you', 1878), (',', 1857)]\n"
     ]
    }
   ],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:29:52.787371Z",
     "start_time": "2024-06-01T12:29:23.745273Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_lemmatization(spam_df)",
   "id": "6a6022ccd225449d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with nltk took 0.5594 seconds\n",
      "Lemmatization with spaCy took 28.3794 seconds\n",
      "Average word length: 82.68\n",
      "5 most frequent words: [('.', 4886), ('to', 2148), ('I', 1956), ('you', 1888), (',', 1871)]\n",
      "Average word length: 81.10\n",
      "5 most frequent words: [('.', 4945), ('I', 3722), ('be', 3260), ('to', 2309), ('you', 2217)]\n"
     ]
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T12:30:22.275341Z",
     "start_time": "2024-06-01T12:29:52.788242Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_stemming(spam_df)",
   "id": "cef6d7d4d6bbac5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming with nltk took 0.9100 seconds\n",
      "Stemming with spaCy took 28.4348 seconds\n",
      "Average word length: 79.26\n",
      "5 most frequent words: [('.', 4886), ('i', 2900), ('to', 2241), ('you', 2228), (',', 1871)]\n",
      "Average word length: 81.10\n",
      "5 most frequent words: [('.', 4945), ('I', 3722), ('be', 3260), ('to', 2309), ('you', 2217)]\n"
     ]
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ",
   "id": "32410af67dfffa8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-01T13:02:06.583335Z",
     "start_time": "2024-06-01T13:02:06.227130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = 'https://x.com/elonmusk'\n",
    "\n",
    "def scrape_text_from_profile(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text_data = []\n",
    "    print(soup.prettify())\n",
    "    profile_name = soup.find('a', class_='profileLink')\n",
    "    if profile_name:\n",
    "        text_data.append(profile_name.get_text(strip=True))\n",
    "\n",
    "    bio_section = soup.find('div', class_='biography')\n",
    "    if bio_section:\n",
    "        text_data.append(bio_section.get_text(strip=True))\n",
    "\n",
    "    # Print the extracted text data\n",
    "    for text in text_data:\n",
    "        print(text)\n",
    "\n",
    "\n",
    "scrape_text_from_profile(url)\n"
   ],
   "id": "2dc7cb148a7cd0be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<head>\n",
      " <title>\n",
      "  x.com\n",
      " </title>\n",
      " <meta content=\"0; url = https://twitter.com/x/migrate?tok=7b2265223a222f656c6f6e6d75736b222c2274223a313731373234363932367d1f940fd969ef86e74bd33a8f8906a4f5\" http-equiv=\"refresh\"/>\n",
      " <meta charset=\"utf-8\"/>\n",
      " <meta content=\"width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover\" name=\"viewport\"/>\n",
      " <link href=\"//abs.twimg.com\" rel=\"preconnect\"/>\n",
      " <link href=\"//abs.twimg.com\" rel=\"dns-prefetch\"/>\n",
      " <link href=\"//api.twitter.com\" rel=\"preconnect\"/>\n",
      " <link href=\"//api.twitter.com\" rel=\"dns-prefetch\"/>\n",
      " <link href=\"//api.x.com\" rel=\"preconnect\"/>\n",
      " <link href=\"//api.x.com\" rel=\"dns-prefetch\"/>\n",
      " <link href=\"//pbs.twimg.com\" rel=\"preconnect\"/>\n",
      " <link href=\"//pbs.twimg.com\" rel=\"dns-prefetch\"/>\n",
      " <link href=\"//t.co\" rel=\"preconnect\"/>\n",
      " <link href=\"//t.co\" rel=\"dns-prefetch\"/>\n",
      " <meta content=\"https://twitter3e4tixl4xyajtrzo62zg5vztmjuricljdp2c5kshju4avyoid.onion/\" http-equiv=\"onion-location\">\n",
      "  <meta content=\"2231777543\" property=\"fb:app_id\">\n",
      "   <meta content=\"X (formerly Twitter)\" property=\"og:site_name\"/>\n",
      "   <meta content=\"600dQ0pZYsH2xOFt4hYmf5f5NpjCbWE_qk5Y04dErYM\" name=\"google-site-verification\"/>\n",
      "   <meta content=\"x6sdcc8b5ju3bh8nbm59eswogvg6t1\" name=\"facebook-domain-verification\"/>\n",
      "   <meta content=\"yes\" name=\"mobile-web-app-capable\"/>\n",
      "   <meta content=\"Twitter\" name=\"apple-mobile-web-app-title\"/>\n",
      "   <meta content=\"white\" name=\"apple-mobile-web-app-status-bar-style\"/>\n",
      "   <link href=\"/opensearch.xml\" rel=\"search\" title=\"Twitter\" type=\"application/opensearchdescription+xml\">\n",
      "    <link href=\"https://abs.twimg.com/responsive-web/client-web/icon-ios.77d25eba.png\" rel=\"apple-touch-icon\" sizes=\"192x192\">\n",
      "     <meta content=\"AUVDWo1JpbjI22xjTe5JOvTAWuW9bK41CpxYxCeCjH97mEVp7rtiHcvdOaUksJrG\" name=\"twitter-site-verification\"/>\n",
      "     <link crossorigin=\"use-credentials\" href=\"/manifest.json\" rel=\"manifest\">\n",
      "      <link color=\"#1D9BF0\" href=\"https://abs.twimg.com/responsive-web/client-web/icon-svg.ea5ff4aa.svg\" rel=\"mask-icon\" sizes=\"any\">\n",
      "       <link href=\"https://abs.twimg.com/favicons/twitter-pip.3.ico\" rel=\"shortcut icon\">\n",
      "        <meta content=\"#000000\" name=\"theme-color\"/>\n",
      "        <script charset=\"utf-8\" nonce=\"NThhMGMyOTktZmI2My00M2NjLWI1NjItYWU1YTY4YjM1Njdi\" type=\"text/javascript\">\n",
      "         document.location = \"https://twitter.com/x/migrate?tok=7b2265223a222f656c6f6e6d75736b222c2274223a313731373234363932367d1f940fd969ef86e74bd33a8f8906a4f5\"\n",
      "        </script>\n",
      "       </link>\n",
      "      </link>\n",
      "     </link>\n",
      "    </link>\n",
      "   </link>\n",
      "  </meta>\n",
      " </meta>\n",
      "</head>\n",
      "<body style=\"background: #000\">\n",
      "</body>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 218
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5ebb28f68f490e04"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
