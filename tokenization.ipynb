{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:48.874125Z",
     "start_time": "2024-06-02T17:13:48.868326Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "outputs": [],
   "execution_count": 211
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## *All the statics are printed to the console and all the data manipulation of nlp are stored as csv and xlsx files*",
   "id": "9a3a1473752e65db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## *Statistics:*",
   "id": "fe949615f481505d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:49.186002Z",
     "start_time": "2024-06-02T17:13:49.178789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(csv_file_path):\n",
    "    df = pd.read_csv(csv_file_path, encoding='latin-1')\n",
    "    df.columns = ['v1', 'v2', 'v3', 'v4', 'v5']\n",
    "    return df"
   ],
   "id": "f2bab60cc46b73c1",
   "outputs": [],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:49.216781Z",
     "start_time": "2024-06-02T17:13:49.202953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_total_sms(df: pd.DataFrame):\n",
    "    total_messages = df.shape[0]\n",
    "    print(f'Total number of SMS messages: {total_messages}')\n",
    "    \n",
    "def print_spam_ham_ratio(df: pd.DataFrame):\n",
    "    spam_count = df[df['v1'] == 'spam'].shape[0]\n",
    "    ham_count = df[df['v1'] == 'ham'].shape[0]\n",
    "    print(f'Spam to ham ratio: {spam_count / ham_count:.2f}')\n",
    "\n",
    "def average_word_length(df: pd.DataFrame, column_name):\n",
    "    df['word_length'] = df[column_name].apply(lambda x: len(x))\n",
    "    avg_word_length = df['word_length'].mean()\n",
    "    print(f'Average word length: {avg_word_length:.2f}')\n",
    "    \n",
    "def most_common_words(df: pd.DataFrame, column_name):\n",
    "    all_words = ' '.join(df[column_name]).split()\n",
    "    most_common = Counter(all_words).most_common(5)\n",
    "    print('5 most frequent words:', most_common)\n",
    "\n",
    "def number_of_words_once(df: pd.DataFrame, column_name):\n",
    "    all_words = ' '.join(df[column_name]).split()\n",
    "    word_counts = Counter(all_words)\n",
    "    words_once = sum(1 for count in word_counts.values() if count == 1)\n",
    "    print(f'Number of words that only appear once: {words_once}')\n",
    "\n",
    "def print_basic_statistics(df: pd.DataFrame):\n",
    "    print_total_sms(df)\n",
    "    print_spam_ham_ratio(df)\n",
    "    average_word_length(df, 'v2')\n",
    "    most_common_words(df, 'v2')\n",
    "    number_of_words_once(df, 'v2')\n",
    "\n",
    "def print_statistics_after_applying_technique(df: pd.DataFrame, column_name):\n",
    "    average_word_length(df, column_name)\n",
    "    most_common_words(df, column_name)\n",
    "    number_of_words_once(df, column_name)\n"
   ],
   "id": "262154602765fb2f",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *NLP Tokenization and Lemmatization:*\n",
   "id": "80002c0941fb8975"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.493352Z",
     "start_time": "2024-06-02T17:13:49.230911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ],
   "id": "68202143644bbfe2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 214
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *NLTK:*",
   "id": "907fa41d7dc7de9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.501031Z",
     "start_time": "2024-06-02T17:13:50.496111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_nltk(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def lemmatize_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def stem_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return [stemmer.stem(token) for token in tokens]\n"
   ],
   "id": "fd0cdbe042093853",
   "outputs": [],
   "execution_count": 215
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *spaCy:*",
   "id": "97103cb2271be2d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.516821Z",
     "start_time": "2024-06-02T17:13:50.502581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]\n",
    "\n",
    "def lemmatize_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc]\n"
   ],
   "id": "78f05f00ae660424",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# *Time Analysis:*",
   "id": "4fa6b918536d971c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.524236Z",
     "start_time": "2024-06-02T17:13:50.519024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_tokenization_time(df, tokenizer, tokenizer_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{tokenizer_name}_tokens'\n",
    "    df[col_name] = df['v2'].apply(tokenizer)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Tokenization with {tokenizer_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "d210a6dccd098873",
   "outputs": [],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.531054Z",
     "start_time": "2024-06-02T17:13:50.525653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_lemmatization_time(df, lemmatization, lemmatization_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{lemmatization_name}_lemmatization'\n",
    "    df[col_name] = df['v2'].apply(lemmatization)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Lemmatization with {lemmatization_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "4306db45581eaf76",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.538604Z",
     "start_time": "2024-06-02T17:13:50.532723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_stemming_time(df, stemming, stemming_name):\n",
    "    start_time = time.time()\n",
    "    col_name = f'{stemming_name}_stemming'\n",
    "    df[col_name] = df['v2'].apply(stemming)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Stemming with {stemming_name} took {elapsed_time:.4f} seconds\")\n"
   ],
   "id": "7c7165996b6fee3d",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.546632Z",
     "start_time": "2024-06-02T17:13:50.540112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_tokenize(df: pd.DataFrame):\n",
    "    analyze_tokenization_time(df, tokenize_nltk, 'nltk')\n",
    "    analyze_tokenization_time(df, tokenize_spacy, 'spaCy')\n",
    "    printed_df = df.copy()\n",
    "    printed_df['nltk_tokens'] = printed_df['nltk_tokens'].apply(lambda x: ' '.join(x))\n",
    "    printed_df['spaCy_tokens'] = printed_df['spaCy_tokens'].apply(lambda x: ' '.join(x))\n",
    "    print_statistics_after_applying_technique(printed_df, 'nltk_tokens')\n",
    "    print_statistics_after_applying_technique(printed_df, 'spaCy_tokens')\n",
    "    df.to_csv('spam_tokenized.csv')"
   ],
   "id": "81a9eaff3359b67b",
   "outputs": [],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.554460Z",
     "start_time": "2024-06-02T17:13:50.548424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_lemmatization(df: pd.DataFrame):\n",
    "    analyze_lemmatization_time(df, lemmatize_nltk, 'nltk')\n",
    "    analyze_lemmatization_time(df, lemmatize_spacy, 'spaCy')\n",
    "    printed_df = df.copy()\n",
    "    printed_df['nltk_lemmatization'] = printed_df['nltk_lemmatization'].apply(lambda x: ' '.join(x))\n",
    "    printed_df['spaCy_lemmatization'] = printed_df['spaCy_lemmatization'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    print_statistics_after_applying_technique(printed_df, 'nltk_lemmatization')\n",
    "    print_statistics_after_applying_technique(printed_df, 'spaCy_lemmatization')\n",
    "\n",
    "    df.to_csv('spam_lemmatize.csv')"
   ],
   "id": "85b30e5707d4165f",
   "outputs": [],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.561443Z",
     "start_time": "2024-06-02T17:13:50.555541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_and_stemming(df: pd.DataFrame):\n",
    "    analyze_stemming_time(df, stem_nltk, 'nltk')\n",
    "    analyze_stemming_time(df, lemmatize_spacy, 'spaCy')\n",
    "    printed_df = df.copy()\n",
    "    printed_df['nltk_stemming'] = printed_df['nltk_stemming'].apply(lambda x: ' '.join(x))\n",
    "    printed_df['spaCy_stemming'] = printed_df['spaCy_stemming'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    print_statistics_after_applying_technique(printed_df, 'nltk_stemming')\n",
    "    print_statistics_after_applying_technique(printed_df, 'spaCy_stemming')\n",
    "\n",
    "    df.to_csv('spam_stemming.csv')"
   ],
   "id": "63fb8486dbc5dec4",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### *NLTK Analyze:*\n",
    "\n",
    "    The output of the NLTK tokenization is a list of tokens for each sentence, it is a simple tokenization process that splits the text by space and punctuation, for example it will split \"I'm\" into \"I\" and \"'m\".\n",
    "    The output of the NLTK lemmatization is a list of lemmatized tokens for each sentence, it is a simple lemmatization process that reduces the words to their base form, In nltk it didnt success to reduce the word to its form, for example \"searching\" stay \"searching\", It didnt preform well.\n",
    "    we used the porter stemmer for stemming the text.  stemming algorithms are known for their simplicity and effectiveness. It applies a series of rules to iteratively strip suffixes from words.\n",
    "    The proccessing speed is very fast, in this case it took 0.3 seconds but the result is not accurate. \n",
    "    It is primarily designed for english.\n",
    "    The complexity for tokenizing/lemmatization/stemming each row is ùëÇ(ùëõ) therefore tokenizing the entire file would be ùëÇ(ùëö‚ãÖùëõ), where ùëö is the average length of the text and ùëõ is the number of rows.\n",
    "    \n",
    "### *spaCy Analyze:*\n",
    "\n",
    "    The output of the spaCy tokenization is a list of tokens for each sentence, it is a more complex tokenization process that takes into account the context of the words, for example it will not split \"I'm\" into \"I\" and \"'m\" and will tokenize it into \"I'm\".\n",
    "    The output of the spaCy lemmatization is a list of lemmatized tokens for each sentence, it is a simple lemmatization process that reduces the words to their base form, for example it will reduce \"running\" to \"run\".\n",
    "    SpaCy doesn't contain any function for stemming as it relies on lemmatization only\n",
    "    The proccessing speed is slower than NLTK, in this case it took 313 seconds and the result is more accurate than NLTK.\n",
    "    it suppport various languages.\n",
    "    The complexity for tokenizing/lemmatization/stemming each row is ùëÇ(ùëõ) therefore tokenizing the entire file would be ùëÇ(ùëö‚ãÖùëõ), where ùëö is the average length of the text and ùëõ is the number of rows."
   ],
   "id": "9769669353e9123d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.613881Z",
     "start_time": "2024-06-02T17:13:50.565075Z"
    }
   },
   "cell_type": "code",
   "source": "spam_df = load_data('spam.csv')",
   "id": "cbcb00659cddede2",
   "outputs": [],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:13:50.698984Z",
     "start_time": "2024-06-02T17:13:50.615891Z"
    }
   },
   "cell_type": "code",
   "source": "print_basic_statistics(spam_df)",
   "id": "6ba727e6f46d0f49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of SMS messages: 5572\n",
      "Spam to ham ratio: 0.15\n",
      "Average word length: 80.12\n",
      "5 most frequent words: [('to', 2134), ('you', 1622), ('I', 1466), ('a', 1327), ('the', 1197)]\n",
      "Number of words that only appear once: 9268\n"
     ]
    }
   ],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:14:27.064382Z",
     "start_time": "2024-06-02T17:13:50.698984Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_tokenize(spam_df)",
   "id": "ad9aa95cc085002a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization with nltk took 0.8406 seconds\n",
      "Tokenization with spaCy took 35.3593 seconds\n",
      "Average word length: 83.26\n",
      "5 most frequent words: [('.', 4886), ('to', 2148), ('I', 1956), ('you', 1888), (',', 1871)]\n",
      "Number of words that only appear once: 6187\n",
      "Average word length: 83.17\n",
      "5 most frequent words: [('.', 4945), ('to', 2148), ('I', 1988), ('you', 1878), (',', 1857)]\n",
      "Number of words that only appear once: 6272\n"
     ]
    }
   ],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:01.468559Z",
     "start_time": "2024-06-02T17:14:27.064382Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_lemmatization(spam_df)",
   "id": "6a6022ccd225449d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization with nltk took 0.8883 seconds\n",
      "Lemmatization with spaCy took 33.2866 seconds\n",
      "Average word length: 82.68\n",
      "5 most frequent words: [('.', 4886), ('to', 2148), ('I', 1956), ('you', 1888), (',', 1871)]\n",
      "Number of words that only appear once: 5903\n",
      "Average word length: 81.10\n",
      "5 most frequent words: [('.', 4945), ('I', 3722), ('be', 3260), ('to', 2309), ('you', 2217)]\n",
      "Number of words that only appear once: 5359\n"
     ]
    }
   ],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:34.198511Z",
     "start_time": "2024-06-02T17:15:01.469566Z"
    }
   },
   "cell_type": "code",
   "source": "load_data_and_stemming(spam_df)",
   "id": "cef6d7d4d6bbac5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming with nltk took 1.6674 seconds\n",
      "Stemming with spaCy took 30.7978 seconds\n",
      "Average word length: 79.26\n",
      "5 most frequent words: [('.', 4886), ('i', 2900), ('to', 2241), ('you', 2228), (',', 1871)]\n",
      "Number of words that only appear once: 4179\n",
      "Average word length: 81.10\n",
      "5 most frequent words: [('.', 4945), ('I', 3722), ('be', 3260), ('to', 2309), ('you', 2217)]\n",
      "Number of words that only appear once: 5359\n"
     ]
    }
   ],
   "execution_count": 227
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ",
   "id": "32410af67dfffa8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:34.917622Z",
     "start_time": "2024-06-02T17:15:34.199527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = 'https://news.sky.com/story/warning-to-uk-politicians-over-risk-of-audio-deepfakes-that-could-derail-the-general-election-13146573'\n",
    "\n",
    "def scrape_text_from_profile(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text_data = []\n",
    "    \n",
    "    scraped_data = soup.find_all('p')\n",
    "    \n",
    "    for text in scraped_data:\n",
    "        text_data.append(text.get_text())\n",
    "        \n",
    "    return text_data\n",
    "      \n",
    "scraped_text = scrape_text_from_profile(url)\n"
   ],
   "id": "2dc7cb148a7cd0be",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### The website that we chose to scrape is sky news\n",
    "#### We chose to perform tokenization and lemmatization using Spacy and stemming from nltk"
   ],
   "id": "32daf89a8a11944a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### *Statistics before text proccesing*   ",
   "id": "526a69d610858776"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:34.926204Z",
     "start_time": "2024-06-02T17:15:34.919131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_scraped_text = pd.DataFrame({\"Scraped Text\": scraped_text})\n",
    "print_statistics_after_applying_technique(df_scraped_text, 'Scraped Text')"
   ],
   "id": "62f12959ff10a3b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 150.46\n",
      "5 most frequent words: [('the', 45), ('to', 43), ('and', 33), ('of', 28), ('in', 17)]\n",
      "Number of words that only appear once: 448\n"
     ]
    }
   ],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:35.457995Z",
     "start_time": "2024-06-02T17:15:34.927714Z"
    }
   },
   "cell_type": "code",
   "source": "df_scraped_text['scraped_tokens'] = df_scraped_text['Scraped Text'].apply(tokenize_spacy)",
   "id": "dffc84dabe85f317",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:35.918598Z",
     "start_time": "2024-06-02T17:15:35.459643Z"
    }
   },
   "cell_type": "code",
   "source": "df_scraped_text['scraped_lemmatization'] = df_scraped_text['Scraped Text'].apply(lemmatize_spacy)",
   "id": "a8db628d80f79d29",
   "outputs": [],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:35.952179Z",
     "start_time": "2024-06-02T17:15:35.919975Z"
    }
   },
   "cell_type": "code",
   "source": "df_scraped_text['scraped_stemming'] = df_scraped_text['Scraped Text'].apply(stem_nltk)",
   "id": "2acd2a2fe67a4165",
   "outputs": [],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:35.961397Z",
     "start_time": "2024-06-02T17:15:35.953190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scraped_df = df_scraped_text.copy()\n",
    "scraped_df['scraped_tokens'] = scraped_df['scraped_tokens'].apply(lambda x: ' '.join(x))\n",
    "scraped_df['scraped_stemming'] = scraped_df['scraped_stemming'].apply(lambda x: ' '.join(x))\n",
    "scraped_df['scraped_lemmatization'] = scraped_df['scraped_lemmatization'].apply(lambda x: ' '.join(x))"
   ],
   "id": "aa19aa0a0c5fb310",
   "outputs": [],
   "execution_count": 233
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:35.970172Z",
     "start_time": "2024-06-02T17:15:35.962520Z"
    }
   },
   "cell_type": "code",
   "source": "print_statistics_after_applying_technique(scraped_df, 'scraped_tokens')",
   "id": "ae53a8bab1e8950",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 154.65\n",
      "5 most frequent words: [(',', 58), ('the', 45), ('.', 44), ('to', 43), ('and', 33)]\n",
      "Number of words that only appear once: 373\n"
     ]
    }
   ],
   "execution_count": 234
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:35.978863Z",
     "start_time": "2024-06-02T17:15:35.971907Z"
    }
   },
   "cell_type": "code",
   "source": "print_statistics_after_applying_technique(scraped_df, 'scraped_stemming')",
   "id": "9b41b5eb1b35f101",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 135.98\n",
      "5 most frequent words: [(',', 58), ('the', 52), ('.', 44), ('to', 43), ('and', 34)]\n",
      "Number of words that only appear once: 303\n"
     ]
    }
   ],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:35.987466Z",
     "start_time": "2024-06-02T17:15:35.981213Z"
    }
   },
   "cell_type": "code",
   "source": "print_statistics_after_applying_technique(scraped_df, 'scraped_lemmatization')",
   "id": "528677ed0ebb5259",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 148.57\n",
      "5 most frequent words: [(',', 58), ('be', 54), ('the', 52), ('.', 44), ('to', 43)]\n",
      "Number of words that only appear once: 310\n"
     ]
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## *WhatsApp Analysis*",
   "id": "34d0aa640b4c265d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:35.992504Z",
     "start_time": "2024-06-02T17:15:35.988553Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e5e35664f99284f",
   "outputs": [],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.000562Z",
     "start_time": "2024-06-02T17:15:35.992504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('_chat.txt', 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "data = []\n",
    "for row in content.split('\\n'):\n",
    "    data.append(row[row.rfind(':') + 1:])\n",
    "\n",
    "whatsapp_df = pd.DataFrame({\"whatsapp_text\":data})\n"
   ],
   "id": "ef092e64bccf565a",
   "outputs": [],
   "execution_count": 237
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.009420Z",
     "start_time": "2024-06-02T17:15:36.002209Z"
    }
   },
   "cell_type": "code",
   "source": "print_statistics_after_applying_technique(whatsapp_df, 'whatsapp_text')",
   "id": "5efb2eb703622562",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 25.33\n",
      "5 most frequent words: [('◊ê◊†◊ô', 30), ('◊ê◊™', 29), ('◊ú◊ê', 26), ('◊ú◊ô', 22), ('◊ñ◊î', 18)]\n",
      "Number of words that only appear once: 501\n"
     ]
    }
   ],
   "execution_count": 238
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.035652Z",
     "start_time": "2024-06-02T17:15:36.010928Z"
    }
   },
   "cell_type": "code",
   "source": "whatsapp_df['scraped_tokens'] = whatsapp_df['whatsapp_text'].apply(tokenize_nltk)",
   "id": "5c1af04ef585e772",
   "outputs": [],
   "execution_count": 239
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.069295Z",
     "start_time": "2024-06-02T17:15:36.039865Z"
    }
   },
   "cell_type": "code",
   "source": "whatsapp_df['scraped_lemmatization'] = whatsapp_df['whatsapp_text'].apply(lemmatize_nltk)",
   "id": "d5ab9013290aaea0",
   "outputs": [],
   "execution_count": 240
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.109538Z",
     "start_time": "2024-06-02T17:15:36.069295Z"
    }
   },
   "cell_type": "code",
   "source": "whatsapp_df['scraped_stemming'] = whatsapp_df['whatsapp_text'].apply(stem_nltk)",
   "id": "ee138f3fdca9fc8b",
   "outputs": [],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.121083Z",
     "start_time": "2024-06-02T17:15:36.111067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whatsapp_df_copy = whatsapp_df.copy()\n",
    "whatsapp_df_copy['scraped_tokens'] = whatsapp_df_copy['scraped_tokens'].apply(lambda x: ' '.join(x))\n",
    "whatsapp_df_copy['scraped_stemming'] = whatsapp_df_copy['scraped_stemming'].apply(lambda x: ' '.join(x))\n",
    "whatsapp_df_copy['scraped_lemmatization'] = whatsapp_df_copy['scraped_lemmatization'].apply(lambda x: ' '.join(x))"
   ],
   "id": "f51c2e91b4ecb3c4",
   "outputs": [],
   "execution_count": 242
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.129741Z",
     "start_time": "2024-06-02T17:15:36.123105Z"
    }
   },
   "cell_type": "code",
   "source": "print_statistics_after_applying_technique(whatsapp_df_copy, 'scraped_tokens')",
   "id": "611ab6dc7042ceb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 24.64\n",
      "5 most frequent words: [('?', 43), ('◊ê◊†◊ô', 30), ('◊ê◊™', 29), ('◊ú◊ê', 26), ('◊ú◊ô', 22)]\n",
      "Number of words that only appear once: 479\n"
     ]
    }
   ],
   "execution_count": 243
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.138435Z",
     "start_time": "2024-06-02T17:15:36.131264Z"
    }
   },
   "cell_type": "code",
   "source": "print_statistics_after_applying_technique(whatsapp_df_copy, 'scraped_stemming')",
   "id": "ea088c4c911d57e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 24.51\n",
      "5 most frequent words: [('?', 43), ('◊ê◊†◊ô', 30), ('◊ê◊™', 29), ('◊ú◊ê', 26), ('◊ú◊ô', 22)]\n",
      "Number of words that only appear once: 476\n"
     ]
    }
   ],
   "execution_count": 244
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.146823Z",
     "start_time": "2024-06-02T17:15:36.139442Z"
    }
   },
   "cell_type": "code",
   "source": "print_statistics_after_applying_technique(whatsapp_df_copy, 'scraped_lemmatization')",
   "id": "1f3dc14aa43c1773",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word length: 24.63\n",
      "5 most frequent words: [('?', 43), ('◊ê◊†◊ô', 30), ('◊ê◊™', 29), ('◊ú◊ê', 26), ('◊ú◊ô', 22)]\n",
      "Number of words that only appear once: 479\n"
     ]
    }
   ],
   "execution_count": 245
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-02T17:15:36.250838Z",
     "start_time": "2024-06-02T17:15:36.148378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "whatsapp_df_copy.drop(columns=\"word_length\", inplace=True)\n",
    "whatsapp_df_copy.to_excel('whatsapp_df.xlsx')"
   ],
   "id": "4b2564e4eb50e7ca",
   "outputs": [],
   "execution_count": 246
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### After analyzing the results of the whatsapp scraped text we can infer that both nltk and spacy don't support the hebrew language",
   "id": "1446c31363cb4c9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2f117009cda72204"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
